{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marcinwolter/Machine-learning-KISD-2023/blob/main/actor_critic_cartpole.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozshgd3tAlaE"
      },
      "source": [
        "# Actor Critic Method\n",
        "\n",
        "**Author:** [Apoorv Nandan](https://twitter.com/NandanApoorv)<br>\n",
        "**Date created:** 2020/05/13<br>\n",
        "**Last modified:** 2020/05/13<br>\n",
        "**Description:** Implement Actor Critic Method in CartPole environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0n4O8LgMAlaG"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "This script shows an implementation of Actor Critic method on CartPole-V0 environment.\n",
        "\n",
        "### Actor Critic Method\n",
        "\n",
        "As an agent takes actions and moves through an environment, it learns to map\n",
        "the observed state of the environment to two possible outputs:\n",
        "\n",
        "1. Recommended action: A probability value for each action in the action space.\n",
        "   The part of the agent responsible for this output is called the **actor**.\n",
        "2. Estimated rewards in the future: Sum of all rewards it expects to receive in the\n",
        "   future. The part of the agent responsible for this output is the **critic**.\n",
        "\n",
        "Agent and Critic learn to perform their tasks, such that the recommended actions\n",
        "from the actor maximize the rewards.\n",
        "\n",
        "### CartPole-V0\n",
        "\n",
        "A pole is attached to a cart placed on a frictionless track. The agent has to apply\n",
        "force to move the cart. It is rewarded for every time step the pole\n",
        "remains upright. The agent, therefore, must learn to keep the pole from falling over.\n",
        "\n",
        "### References\n",
        "\n",
        "- [CartPole](http://www.derongliu.org/adp/adp-cdrom/Barto1983.pdf)\n",
        "- [Actor Critic Method](https://hal.inria.fr/hal-00840470/document)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9tXRhXNAlaH"
      },
      "source": [
        "## Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NYzp4i_1AlaI",
        "outputId": "bca93579-0b03-4e79-d226-77ce7c2648f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libfontenc1 libxfont2 libxkbfile1 libxtst6 libxxf86dga1 x11-xkb-utils\n",
            "  xfonts-base xfonts-encodings xfonts-utils xserver-common\n",
            "Suggested packages:\n",
            "  mesa-utils\n",
            "The following NEW packages will be installed:\n",
            "  libfontenc1 libxfont2 libxkbfile1 libxtst6 libxxf86dga1 x11-utils\n",
            "  x11-xkb-utils xfonts-base xfonts-encodings xfonts-utils xserver-common xvfb\n",
            "0 upgraded, 12 newly installed, 0 to remove and 24 not upgraded.\n",
            "Need to get 7,920 kB of archives.\n",
            "After this operation, 12.7 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu focal/main amd64 libfontenc1 amd64 1:1.1.4-0ubuntu1 [14.0 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu focal/main amd64 libxfont2 amd64 1:2.0.3-1 [91.7 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu focal/main amd64 libxkbfile1 amd64 1:1.1.0-1 [65.3 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu focal/main amd64 libxtst6 amd64 2:1.2.3-1 [12.8 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu focal/main amd64 libxxf86dga1 amd64 2:1.1.5-0ubuntu1 [12.0 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu focal/main amd64 x11-utils amd64 7.7+5 [199 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu focal/main amd64 x11-xkb-utils amd64 7.7+5 [158 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu focal/main amd64 xfonts-encodings all 1:1.0.5-0ubuntu1 [573 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu focal/main amd64 xfonts-utils amd64 1:7.7+6 [91.5 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu focal/main amd64 xfonts-base all 1:1.0.5 [5,896 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 xserver-common all 2:1.20.13-1ubuntu1~20.04.8 [27.2 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 xvfb amd64 2:1.20.13-1ubuntu1~20.04.8 [780 kB]\n",
            "Fetched 7,920 kB in 2s (4,342 kB/s)\n",
            "Selecting previously unselected package libfontenc1:amd64.\n",
            "(Reading database ... 122349 files and directories currently installed.)\n",
            "Preparing to unpack .../00-libfontenc1_1%3a1.1.4-0ubuntu1_amd64.deb ...\n",
            "Unpacking libfontenc1:amd64 (1:1.1.4-0ubuntu1) ...\n",
            "Selecting previously unselected package libxfont2:amd64.\n",
            "Preparing to unpack .../01-libxfont2_1%3a2.0.3-1_amd64.deb ...\n",
            "Unpacking libxfont2:amd64 (1:2.0.3-1) ...\n",
            "Selecting previously unselected package libxkbfile1:amd64.\n",
            "Preparing to unpack .../02-libxkbfile1_1%3a1.1.0-1_amd64.deb ...\n",
            "Unpacking libxkbfile1:amd64 (1:1.1.0-1) ...\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "Preparing to unpack .../03-libxtst6_2%3a1.2.3-1_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1) ...\n",
            "Selecting previously unselected package libxxf86dga1:amd64.\n",
            "Preparing to unpack .../04-libxxf86dga1_2%3a1.1.5-0ubuntu1_amd64.deb ...\n",
            "Unpacking libxxf86dga1:amd64 (2:1.1.5-0ubuntu1) ...\n",
            "Selecting previously unselected package x11-utils.\n",
            "Preparing to unpack .../05-x11-utils_7.7+5_amd64.deb ...\n",
            "Unpacking x11-utils (7.7+5) ...\n",
            "Selecting previously unselected package x11-xkb-utils.\n",
            "Preparing to unpack .../06-x11-xkb-utils_7.7+5_amd64.deb ...\n",
            "Unpacking x11-xkb-utils (7.7+5) ...\n",
            "Selecting previously unselected package xfonts-encodings.\n",
            "Preparing to unpack .../07-xfonts-encodings_1%3a1.0.5-0ubuntu1_all.deb ...\n",
            "Unpacking xfonts-encodings (1:1.0.5-0ubuntu1) ...\n",
            "Selecting previously unselected package xfonts-utils.\n",
            "Preparing to unpack .../08-xfonts-utils_1%3a7.7+6_amd64.deb ...\n",
            "Unpacking xfonts-utils (1:7.7+6) ...\n",
            "Selecting previously unselected package xfonts-base.\n",
            "Preparing to unpack .../09-xfonts-base_1%3a1.0.5_all.deb ...\n",
            "Unpacking xfonts-base (1:1.0.5) ...\n",
            "Selecting previously unselected package xserver-common.\n",
            "Preparing to unpack .../10-xserver-common_2%3a1.20.13-1ubuntu1~20.04.8_all.deb ...\n",
            "Unpacking xserver-common (2:1.20.13-1ubuntu1~20.04.8) ...\n",
            "Selecting previously unselected package xvfb.\n",
            "Preparing to unpack .../11-xvfb_2%3a1.20.13-1ubuntu1~20.04.8_amd64.deb ...\n",
            "Unpacking xvfb (2:1.20.13-1ubuntu1~20.04.8) ...\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1) ...\n",
            "Setting up libxxf86dga1:amd64 (2:1.1.5-0ubuntu1) ...\n",
            "Setting up libfontenc1:amd64 (1:1.1.4-0ubuntu1) ...\n",
            "Setting up xfonts-encodings (1:1.0.5-0ubuntu1) ...\n",
            "Setting up libxkbfile1:amd64 (1:1.1.0-1) ...\n",
            "Setting up libxfont2:amd64 (1:2.0.3-1) ...\n",
            "Setting up x11-xkb-utils (7.7+5) ...\n",
            "Setting up xfonts-utils (1:7.7+6) ...\n",
            "Setting up xfonts-base (1:1.0.5) ...\n",
            "Setting up x11-utils (7.7+5) ...\n",
            "Setting up xserver-common (2:1.20.13-1ubuntu1~20.04.8) ...\n",
            "Setting up xvfb (2:1.20.13-1ubuntu1~20.04.8) ...\n",
            "Processing triggers for man-db (2.9.1-1) ...\n",
            "Processing triggers for fontconfig (2.13.1-2ubuntu3) ...\n",
            "Processing triggers for libc-bin (2.31-0ubuntu9.9) ...\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyvirtualdisplay==0.2.*\n",
            "  Downloading PyVirtualDisplay-0.2.5-py2.py3-none-any.whl (13 kB)\n",
            "Collecting EasyProcess\n",
            "  Downloading EasyProcess-1.1-py3-none-any.whl (8.7 kB)\n",
            "Installing collected packages: EasyProcess, pyvirtualdisplay\n",
            "Successfully installed EasyProcess-1.1 pyvirtualdisplay-0.2.5\n"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "from gym import wrappers\n",
        "from gym.wrappers.record_video import RecordVideo\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "\n",
        "# install dependencies needed for recording videos\n",
        "!apt-get install -y xvfb x11-utils\n",
        "!pip install pyvirtualdisplay==0.2.*\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=False, size=(1400, 900))\n",
        "_ = display.start()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Configuration parameters for the whole setup\n",
        "seed = 42\n",
        "gamma = 0.99  # Discount factor for past rewards\n",
        "max_steps_per_episode = 10000\n",
        "env = gym.make(\"CartPole-v1\")  # Create the environment\n",
        "env.reset(seed=seed)\n",
        "eps = np.finfo(np.float32).eps.item()  # Smallest number such that 1.0 + eps != 1.0\n"
      ],
      "metadata": {
        "id": "X4NOlXFV6Yel",
        "outputId": "2e2290e9-593d-4929-9688-0f8bdebe1f9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "/usr/local/lib/python3.9/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYS4spFrAlaJ"
      },
      "source": [
        "## Implement Actor Critic network\n",
        "\n",
        "This network learns two functions:\n",
        "\n",
        "1. Actor: This takes as input the state of our environment and returns a\n",
        "probability value for each action in its action space.\n",
        "2. Critic: This takes as input the state of our environment and returns\n",
        "an estimate of total rewards in the future.\n",
        "\n",
        "In our implementation, they share the initial layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "OBXhXpq6AlaJ"
      },
      "outputs": [],
      "source": [
        "num_inputs = 4\n",
        "num_actions = 2\n",
        "num_hidden = 128\n",
        "\n",
        "inputs = layers.Input(shape=(num_inputs,))\n",
        "common = layers.Dense(num_hidden, activation=\"relu\")(inputs)\n",
        "action = layers.Dense(num_actions, activation=\"softmax\")(common)\n",
        "critic = layers.Dense(1)(common)\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=[action, critic])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_video(i, env, TrainNet):\n",
        "\n",
        "    #env = RecordVideo(env, os.path.join(os.getcwd(), \"videos\"+str(i)),  episode_trigger = lambda episode_number: True)\n",
        "\n",
        "    rewards = 0\n",
        "    steps = 0\n",
        "    done = False\n",
        "    observation = env.reset()\n",
        "    while not done:\n",
        "        env.render()\n",
        "        action = TrainNet.get_action(observation, 0)\n",
        "        observation, reward, done, _ = env.step(action)\n",
        "        steps += 1\n",
        "        rewards += reward\n",
        "    print(\"Testing steps: {} rewards {}: \".format(steps, rewards))"
      ],
      "metadata": {
        "id": "A77k0sXwrWl4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sz56aqURAlaJ"
      },
      "source": [
        "## Train\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "wuDyHmdaAlaK",
        "outputId": "df5672a5-ba3c-413c-e0e9-85b6b3fc0535",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/gym/wrappers/monitoring/video_recorder.py:78: DeprecationWarning: \u001b[33mWARN: Recording ability for environment CartPole-v1 initialized with `render_mode=None` is marked as deprecated and will be removed in the future.\u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/wrappers/monitoring/video_recorder.py:101: DeprecationWarning: \u001b[33mWARN: <class 'gym.wrappers.monitoring.video_recorder.VideoRecorder'> is marked as deprecated and will be removed in the future.\u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n",
            "WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x7fb9f8fbe790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x7fb9f8fbe790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running reward: 5.71 at episode 10\n",
            "running reward: 8.46 at episode 20\n",
            "running reward: 11.19 at episode 30\n",
            "running reward: 12.58 at episode 40\n",
            "running reward: 12.76 at episode 50\n",
            "running reward: 13.18 at episode 60\n",
            "running reward: 15.75 at episode 70\n",
            "running reward: 15.62 at episode 80\n",
            "running reward: 17.20 at episode 90\n",
            "running reward: 24.94 at episode 100\n",
            "running reward: 25.84 at episode 110\n",
            "running reward: 25.53 at episode 120\n",
            "running reward: 39.72 at episode 130\n",
            "running reward: 45.55 at episode 140\n",
            "running reward: 58.40 at episode 150\n",
            "running reward: 45.27 at episode 160\n",
            "running reward: 33.67 at episode 170\n",
            "running reward: 28.21 at episode 180\n",
            "running reward: 23.04 at episode 190\n",
            "running reward: 23.33 at episode 200\n",
            "running reward: 27.63 at episode 210\n",
            "running reward: 43.23 at episode 220\n",
            "running reward: 61.32 at episode 230\n",
            "running reward: 71.95 at episode 240\n",
            "running reward: 67.84 at episode 250\n",
            "running reward: 90.30 at episode 260\n",
            "running reward: 117.80 at episode 270\n",
            "running reward: 175.40 at episode 280\n",
            "Solved at episode 284!\n"
          ]
        }
      ],
      "source": [
        "optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
        "huber_loss = keras.losses.Huber()\n",
        "action_probs_history = []\n",
        "critic_value_history = []\n",
        "rewards_history = []\n",
        "running_reward = 0\n",
        "episode_count = 0\n",
        "\n",
        "from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
        "video = VideoRecorder(env, 'training'+'.mp4')\n",
        "while True:  # Run until solved\n",
        "    \n",
        "    #env = RecordVideo(env, os.path.join(os.getcwd(), \"videos\"+str(episode_count)),  episode_trigger = lambda episode_number: True)\n",
        "    #if episode_count % 10 == 0:\n",
        "    #  from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
        "    #  video = VideoRecorder(env, 'training'+str(episode_count)+'.mp4')\n",
        "\n",
        "    state = env.reset()\n",
        "    #img = plt.imshow(env.render(mode='rgb_array')) # only call this once\n",
        "    #env = RecordVideo(env, os.path.join(os.getcwd(), \"videos\"),  episode_trigger = lambda episode_number: True)\n",
        "\n",
        "\n",
        "    episode_reward = 0\n",
        "    with tf.GradientTape() as tape:\n",
        "        for timestep in range(1, max_steps_per_episode):\n",
        "            #img.set_data(env.render(mode='rgb_array')) # just update the data...\n",
        "            #display.display(plt.gcf())\n",
        "            #display.clear_output(wait=True)\n",
        "            env.render(mode='rgb_array')\n",
        "            video.capture_frame()\n",
        "            # env.render(); Adding this line would show the attempts\n",
        "            # of the agent in a pop up window.\n",
        "\n",
        "            state = tf.convert_to_tensor(state)\n",
        "            state = tf.expand_dims(state, 0)\n",
        "\n",
        "            # Predict action probabilities and estimated future rewards\n",
        "            # from environment state\n",
        "            action_probs, critic_value = model(state)\n",
        "            critic_value_history.append(critic_value[0, 0])\n",
        "\n",
        "            # Sample action from action probability distribution\n",
        "            action = np.random.choice(num_actions, p=np.squeeze(action_probs))\n",
        "            action_probs_history.append(tf.math.log(action_probs[0, action]))\n",
        "\n",
        "            # Apply the sampled action in our environment\n",
        "            state, reward, done, _ = env.step(action)\n",
        "            rewards_history.append(reward)\n",
        "            episode_reward += reward\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        # Update running reward to check condition for solving\n",
        "        running_reward = 0.05 * episode_reward + (1 - 0.05) * running_reward\n",
        "\n",
        "        # Calculate expected value from rewards\n",
        "        # - At each timestep what was the total reward received after that timestep\n",
        "        # - Rewards in the past are discounted by multiplying them with gamma\n",
        "        # - These are the labels for our critic\n",
        "        returns = []\n",
        "        discounted_sum = 0\n",
        "        for r in rewards_history[::-1]:\n",
        "            discounted_sum = r + gamma * discounted_sum\n",
        "            returns.insert(0, discounted_sum)\n",
        "\n",
        "        # Normalize\n",
        "        returns = np.array(returns)\n",
        "        returns = (returns - np.mean(returns)) / (np.std(returns) + eps)\n",
        "        returns = returns.tolist()\n",
        "\n",
        "        # Calculating loss values to update our network\n",
        "        history = zip(action_probs_history, critic_value_history, returns)\n",
        "        actor_losses = []\n",
        "        critic_losses = []\n",
        "        for log_prob, value, ret in history:\n",
        "            # At this point in history, the critic estimated that we would get a\n",
        "            # total reward = `value` in the future. We took an action with log probability\n",
        "            # of `log_prob` and ended up recieving a total reward = `ret`.\n",
        "            # The actor must be updated so that it predicts an action that leads to\n",
        "            # high rewards (compared to critic's estimate) with high probability.\n",
        "            diff = ret - value\n",
        "            actor_losses.append(-log_prob * diff)  # actor loss\n",
        "\n",
        "            # The critic must be updated so that it predicts a better estimate of\n",
        "            # the future rewards.\n",
        "            critic_losses.append(\n",
        "                huber_loss(tf.expand_dims(value, 0), tf.expand_dims(ret, 0))\n",
        "            )\n",
        "\n",
        "        # Backpropagation\n",
        "        loss_value = sum(actor_losses) + sum(critic_losses)\n",
        "        grads = tape.gradient(loss_value, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "        # Clear the loss and reward history\n",
        "        action_probs_history.clear()\n",
        "        critic_value_history.clear()\n",
        "        rewards_history.clear()\n",
        "\n",
        "    # Log details\n",
        "    episode_count += 1\n",
        "    if episode_count % 10 == 0:\n",
        "        template = \"running reward: {:.2f} at episode {}\"\n",
        "        print(template.format(running_reward, episode_count))\n",
        "\n",
        "        #make_video(episode_count, env, tape)\n",
        "\n",
        "\n",
        "    if running_reward > 195: # 195:  # Condition to consider the task solved\n",
        "        print(\"Solved at episode {}!\".format(episode_count))\n",
        "        video.close()\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NWwnwFUAlaL"
      },
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "actor_critic_cartpole",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}