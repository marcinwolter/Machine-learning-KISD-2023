{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPkds4j24eKz7l3uobhiekr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marcinwolter/Machine-learning-KISD-2023/blob/main/CartPole.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWWlt7jmmh86",
        "outputId": "f93d99fe-b553-432a-9803-44a08427827e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "episode: 0 episode reward: 14.0 eps: 0.989901 avg reward (last 100): 14.0 episode loss:  0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x7fee98461ca0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x7fee98461ca0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "episode: 100 episode reward: 16.0 eps: 0.980050830419928 avg reward (last 100): 20.722772277227723 episode loss:  477.16934\n",
            "episode: 200 episode reward: 53.0 eps: 0.9702986765411791 avg reward (last 100): 23.198019801980198 episode loss:  195.53236\n",
            "episode: 300 episode reward: 9.0 eps: 0.960643563042708 avg reward (last 100): 22.99009900990099 episode loss:  388.6235\n",
            "episode: 400 episode reward: 19.0 eps: 0.9510845243085565 avg reward (last 100): 21.95049504950495 episode loss:  324.52524\n",
            "episode: 500 episode reward: 12.0 eps: 0.9416206043312847 avg reward (last 100): 21.495049504950494 episode loss:  402.4758\n",
            "episode: 600 episode reward: 17.0 eps: 0.9322508566163586 avg reward (last 100): 21.980198019801982 episode loss:  293.57416\n",
            "episode: 700 episode reward: 33.0 eps: 0.9229743440874912 avg reward (last 100): 23.18811881188119 episode loss:  371.46545\n",
            "episode: 800 episode reward: 22.0 eps: 0.913790138992923 avg reward (last 100): 22.07920792079208 episode loss:  273.8522\n",
            "episode: 900 episode reward: 10.0 eps: 0.9046973228126401 avg reward (last 100): 22.73267326732673 episode loss:  263.85474\n",
            "episode: 1000 episode reward: 13.0 eps: 0.8956949861665088 avg reward (last 100): 24.663366336633665 episode loss:  354.875\n",
            "episode: 1100 episode reward: 37.0 eps: 0.8867822287233291 avg reward (last 100): 24.73267326732673 episode loss:  766.18616\n",
            "episode: 1200 episode reward: 12.0 eps: 0.877958159110793 avg reward (last 100): 25.128712871287128 episode loss:  328.18582\n",
            "episode: 1300 episode reward: 20.0 eps: 0.8692218948263346 avg reward (last 100): 23.08910891089109 episode loss:  590.1203\n",
            "episode: 1400 episode reward: 15.0 eps: 0.8605725621488737 avg reward (last 100): 24.702970297029704 episode loss:  987.48193\n",
            "episode: 1500 episode reward: 17.0 eps: 0.8520092960514319 avg reward (last 100): 24.198019801980198 episode loss:  678.39197\n",
            "episode: 1600 episode reward: 14.0 eps: 0.843531240114621 avg reward (last 100): 23.504950495049506 episode loss:  830.05115\n",
            "episode: 1700 episode reward: 11.0 eps: 0.8351375464409935 avg reward (last 100): 22.980198019801982 episode loss:  598.3663\n",
            "episode: 1800 episode reward: 17.0 eps: 0.8268273755702407 avg reward (last 100): 26.415841584158414 episode loss:  256.8712\n",
            "episode: 1900 episode reward: 61.0 eps: 0.8185998963952398 avg reward (last 100): 24.88118811881188 episode loss:  542.3457\n",
            "episode: 2000 episode reward: 45.0 eps: 0.8104542860789328 avg reward (last 100): 25.980198019801982 episode loss:  774.3456\n",
            "episode: 2100 episode reward: 15.0 eps: 0.802389729972035 avg reward (last 100): 23.88118811881188 episode loss:  532.2275\n",
            "episode: 2200 episode reward: 12.0 eps: 0.7944054215315619 avg reward (last 100): 23.524752475247524 episode loss:  1098.3928\n",
            "episode: 2300 episode reward: 17.0 eps: 0.786500562240163 avg reward (last 100): 22.821782178217823 episode loss:  281.26108\n",
            "episode: 2400 episode reward: 45.0 eps: 0.7786743615262652 avg reward (last 100): 21.03960396039604 episode loss:  591.3152\n",
            "episode: 2500 episode reward: 14.0 eps: 0.7709260366850046 avg reward (last 100): 24.534653465346533 episode loss:  308.69907\n",
            "episode: 2600 episode reward: 15.0 eps: 0.7632548127999497 avg reward (last 100): 27.99009900990099 episode loss:  1179.8273\n",
            "episode: 2700 episode reward: 24.0 eps: 0.755659922665597 avg reward (last 100): 24.485148514851485 episode loss:  284.7485\n",
            "episode: 2800 episode reward: 28.0 eps: 0.7481406067106477 avg reward (last 100): 24.524752475247524 episode loss:  587.19336\n",
            "episode: 2900 episode reward: 35.0 eps: 0.7406961129220389 avg reward (last 100): 22.980198019801982 episode loss:  615.4187\n",
            "episode: 3000 episode reward: 19.0 eps: 0.7333256967697342 avg reward (last 100): 25.14851485148515 episode loss:  508.6274\n",
            "episode: 3100 episode reward: 56.0 eps: 0.7260286211322649 avg reward (last 100): 23.594059405940595 episode loss:  327.86227\n",
            "episode: 3200 episode reward: 40.0 eps: 0.7188041562230073 avg reward (last 100): 23.653465346534652 episode loss:  265.37216\n",
            "episode: 3300 episode reward: 17.0 eps: 0.7116515795171979 avg reward (last 100): 23.84158415841584 episode loss:  240.11098\n",
            "episode: 3400 episode reward: 40.0 eps: 0.7045701756796721 avg reward (last 100): 30.455445544554454 episode loss:  198.20221\n",
            "episode: 3500 episode reward: 12.0 eps: 0.6975592364933232 avg reward (last 100): 26.00990099009901 episode loss:  1155.8102\n",
            "episode: 3600 episode reward: 13.0 eps: 0.6906180607882736 avg reward (last 100): 24.277227722772277 episode loss:  294.03946\n",
            "episode: 3700 episode reward: 20.0 eps: 0.6837459543717475 avg reward (last 100): 25.89108910891089 episode loss:  413.15625\n",
            "episode: 3800 episode reward: 44.0 eps: 0.6769422299586498 avg reward (last 100): 23.297029702970296 episode loss:  320.03934\n",
            "episode: 3900 episode reward: 22.0 eps: 0.6702062071028237 avg reward (last 100): 25.613861386138613 episode loss:  402.37033\n",
            "episode: 4000 episode reward: 12.0 eps: 0.6635372121290035 avg reward (last 100): 23.217821782178216 episode loss:  326.81216\n",
            "episode: 4100 episode reward: 49.0 eps: 0.656934578065436 avg reward (last 100): 23.455445544554454 episode loss:  570.97015\n",
            "episode: 4200 episode reward: 26.0 eps: 0.6503976445771799 avg reward (last 100): 25.77227722772277 episode loss:  315.15656\n",
            "episode: 4300 episode reward: 15.0 eps: 0.6439257579000631 avg reward (last 100): 25.346534653465348 episode loss:  430.09567\n",
            "episode: 4400 episode reward: 48.0 eps: 0.6375182707752978 avg reward (last 100): 24.97029702970297 episode loss:  630.54364\n",
            "episode: 4500 episode reward: 34.0 eps: 0.6311745423847503 avg reward (last 100): 24.435643564356436 episode loss:  416.6986\n",
            "episode: 4600 episode reward: 18.0 eps: 0.6248939382868509 avg reward (last 100): 23.019801980198018 episode loss:  348.97635\n",
            "episode: 4700 episode reward: 35.0 eps: 0.6186758303531436 avg reward (last 100): 27.623762376237625 episode loss:  479.66113\n",
            "episode: 4800 episode reward: 24.0 eps: 0.6125195967054651 avg reward (last 100): 25.514851485148515 episode loss:  915.9403\n",
            "episode: 4900 episode reward: 18.0 eps: 0.6064246216537515 avg reward (last 100): 22.326732673267326 episode loss:  537.838\n",
            "episode: 5000 episode reward: 45.0 eps: 0.6003902956344622 avg reward (last 100): 22.801980198019802 episode loss:  392.04565\n",
            "episode: 5100 episode reward: 9.0 eps: 0.5944160151496163 avg reward (last 100): 24.762376237623762 episode loss:  674.6289\n",
            "episode: 5200 episode reward: 18.0 eps: 0.5885011827064386 avg reward (last 100): 20.059405940594058 episode loss:  349.38513\n",
            "episode: 5300 episode reward: 13.0 eps: 0.5826452067575998 avg reward (last 100): 21.356435643564357 episode loss:  207.35628\n",
            "episode: 5400 episode reward: 18.0 eps: 0.5768475016420588 avg reward (last 100): 22.762376237623762 episode loss:  831.4672\n",
            "episode: 5500 episode reward: 31.0 eps: 0.5711074875264902 avg reward (last 100): 22.584158415841586 episode loss:  264.18457\n",
            "episode: 5600 episode reward: 79.0 eps: 0.5654245903472923 avg reward (last 100): 26.752475247524753 episode loss:  522.53625\n",
            "episode: 5700 episode reward: 14.0 eps: 0.5597982417531768 avg reward (last 100): 30.871287128712872 episode loss:  251.5449\n",
            "episode: 5800 episode reward: 18.0 eps: 0.5542278790483262 avg reward (last 100): 24.03960396039604 episode loss:  624.86694\n",
            "episode: 5900 episode reward: 17.0 eps: 0.5487129451361167 avg reward (last 100): 22.198019801980198 episode loss:  338.301\n",
            "episode: 6000 episode reward: 11.0 eps: 0.543252888463407 avg reward (last 100): 22.940594059405942 episode loss:  466.13702\n",
            "episode: 6100 episode reward: 14.0 eps: 0.5378471629653735 avg reward (last 100): 24.475247524752476 episode loss:  388.03635\n",
            "episode: 6200 episode reward: 10.0 eps: 0.5324952280108983 avg reward (last 100): 22.554455445544555 episode loss:  254.13248\n",
            "episode: 6300 episode reward: 13.0 eps: 0.5271965483485016 avg reward (last 100): 24.782178217821784 episode loss:  583.2714\n",
            "episode: 6400 episode reward: 26.0 eps: 0.5219505940528081 avg reward (last 100): 22.514851485148515 episode loss:  428.78232\n",
            "episode: 6500 episode reward: 11.0 eps: 0.5167568404715517 avg reward (last 100): 26.207920792079207 episode loss:  746.6367\n",
            "episode: 6600 episode reward: 55.0 eps: 0.5116147681731024 avg reward (last 100): 28.633663366336634 episode loss:  476.252\n",
            "episode: 6700 episode reward: 13.0 eps: 0.5065238628945193 avg reward (last 100): 26.366336633663366 episode loss:  323.53882\n",
            "episode: 6800 episode reward: 50.0 eps: 0.501483615490118 avg reward (last 100): 25.792079207920793 episode loss:  468.9528\n",
            "episode: 6900 episode reward: 51.0 eps: 0.4964935218805499 avg reward (last 100): 29.99009900990099 episode loss:  616.9827\n",
            "episode: 7000 episode reward: 29.0 eps: 0.49155308300238854 avg reward (last 100): 30.22772277227723 episode loss:  735.2906\n",
            "episode: 7100 episode reward: 12.0 eps: 0.48666180475821974 avg reward (last 100): 28.425742574257427 episode loss:  85.692566\n",
            "episode: 7200 episode reward: 37.0 eps: 0.48181919796722483 avg reward (last 100): 27.574257425742573 episode loss:  668.5513\n",
            "episode: 7300 episode reward: 42.0 eps: 0.477024778316258 avg reward (last 100): 29.613861386138613 episode loss:  536.60504\n",
            "episode: 7400 episode reward: 20.0 eps: 0.47227806631140934 avg reward (last 100): 24.366336633663366 episode loss:  530.4816\n",
            "episode: 7500 episode reward: 25.0 eps: 0.4675785872300506 avg reward (last 100): 26.178217821782177 episode loss:  300.58694\n",
            "episode: 7600 episode reward: 67.0 eps: 0.4629258710733578 avg reward (last 100): 28.93069306930693 episode loss:  301.67038\n",
            "episode: 7700 episode reward: 68.0 eps: 0.45831945251930506 avg reward (last 100): 26.77227722772277 episode loss:  525.7143\n",
            "episode: 7800 episode reward: 14.0 eps: 0.45375887087612954 avg reward (last 100): 27.673267326732674 episode loss:  1094.6763\n",
            "episode: 7900 episode reward: 99.0 eps: 0.4492436700362556 avg reward (last 100): 25.544554455445546 episode loss:  644.2079\n",
            "episode: 8000 episode reward: 42.0 eps: 0.44477339843067976 avg reward (last 100): 33.32673267326733 episode loss:  330.19254\n",
            "episode: 8100 episode reward: 28.0 eps: 0.4403476089838086 avg reward (last 100): 26.346534653465348 episode loss:  362.88382\n",
            "episode: 8200 episode reward: 13.0 eps: 0.43596585906874646 avg reward (last 100): 28.594059405940595 episode loss:  268.31754\n",
            "episode: 8300 episode reward: 31.0 eps: 0.43162771046302806 avg reward (last 100): 29.415841584158414 episode loss:  138.34999\n",
            "episode: 8400 episode reward: 24.0 eps: 0.4273327293047919 avg reward (last 100): 25.712871287128714 episode loss:  332.4288\n",
            "episode: 8500 episode reward: 15.0 eps: 0.42308048604938847 avg reward (last 100): 23.91089108910891 episode loss:  537.4382\n",
            "episode: 8600 episode reward: 15.0 eps: 0.41887055542642126 avg reward (last 100): 26.465346534653467 episode loss:  384.08734\n",
            "episode: 8700 episode reward: 23.0 eps: 0.4147025163972157 avg reward (last 100): 30.534653465346533 episode loss:  365.72842\n",
            "episode: 8800 episode reward: 16.0 eps: 0.4105759521127113 avg reward (last 100): 29.92079207920792 episode loss:  479.6492\n",
            "episode: 8900 episode reward: 13.0 eps: 0.4064904498717701 avg reward (last 100): 32.08910891089109 episode loss:  233.80664\n",
            "episode: 9000 episode reward: 18.0 eps: 0.4024456010799042 avg reward (last 100): 27.485148514851485 episode loss:  840.389\n",
            "episode: 9100 episode reward: 24.0 eps: 0.3984410012084108 avg reward (last 100): 31.821782178217823 episode loss:  603.31287\n",
            "episode: 9200 episode reward: 33.0 eps: 0.3944762497539148 avg reward (last 100): 29.88118811881188 episode loss:  415.55588\n",
            "episode: 9300 episode reward: 74.0 eps: 0.39055095019831576 avg reward (last 100): 27.821782178217823 episode loss:  484.14417\n",
            "episode: 9400 episode reward: 12.0 eps: 0.3866647099691295 avg reward (last 100): 27.07920792079208 episode loss:  175.81085\n",
            "episode: 9500 episode reward: 33.0 eps: 0.3828171404002276 avg reward (last 100): 27.04950495049505 episode loss:  568.57166\n",
            "episode: 9600 episode reward: 15.0 eps: 0.3790078566929674 avg reward (last 100): 27.06930693069307 episode loss:  990.35046\n",
            "episode: 9700 episode reward: 61.0 eps: 0.37523647787770664 avg reward (last 100): 30.168316831683168 episode loss:  325.17096\n",
            "episode: 9800 episode reward: 20.0 eps: 0.37150262677570317 avg reward (last 100): 30.14851485148515 episode loss:  530.76544\n",
            "episode: 9900 episode reward: 10.0 eps: 0.36780592996139233 avg reward (last 100): 26.00990099009901 episode loss:  174.6033\n",
            "avg reward for last 100 episodes: 28.10891089108911\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/gym/wrappers/monitoring/video_recorder.py:78: DeprecationWarning: \u001b[33mWARN: Recording ability for environment CartPole-v0 initialized with `render_mode=None` is marked as deprecated and will be removed in the future.\u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/core.py:49: DeprecationWarning: \u001b[33mWARN: You are calling render method, but you didn't specified the argument render_mode at environment initialization. To maintain backward compatibility, the environment will render in human mode.\n",
            "If you want to render in human mode, initialize the environment in this way: gym.make('EnvName', render_mode='human') and don't call the render method.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing steps: 97 rewards 97.0: \n",
            "episode: 0 episode reward: 30.0 eps: 0.989901 avg reward (last 100): 30.0 episode loss:  0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "episode: 100 episode reward: 33.0 eps: 0.980050830419928 avg reward (last 100): 22.217821782178216 episode loss:  107.35336\n",
            "episode: 200 episode reward: 31.0 eps: 0.9702986765411791 avg reward (last 100): 23.247524752475247 episode loss:  110.16057\n",
            "episode: 300 episode reward: 35.0 eps: 0.960643563042708 avg reward (last 100): 23.138613861386137 episode loss:  242.84854\n",
            "episode: 400 episode reward: 25.0 eps: 0.9510845243085565 avg reward (last 100): 22.584158415841586 episode loss:  150.98892\n",
            "episode: 500 episode reward: 35.0 eps: 0.9416206043312847 avg reward (last 100): 20.801980198019802 episode loss:  233.35246\n",
            "episode: 600 episode reward: 16.0 eps: 0.9322508566163586 avg reward (last 100): 23.346534653465348 episode loss:  222.2296\n",
            "episode: 700 episode reward: 24.0 eps: 0.9229743440874912 avg reward (last 100): 23.584158415841586 episode loss:  152.22426\n",
            "episode: 800 episode reward: 21.0 eps: 0.913790138992923 avg reward (last 100): 22.217821782178216 episode loss:  295.99744\n",
            "episode: 900 episode reward: 41.0 eps: 0.9046973228126401 avg reward (last 100): 23.88118811881188 episode loss:  289.98267\n",
            "episode: 1000 episode reward: 23.0 eps: 0.8956949861665088 avg reward (last 100): 22.524752475247524 episode loss:  151.85555\n",
            "episode: 1100 episode reward: 20.0 eps: 0.8867822287233291 avg reward (last 100): 21.673267326732674 episode loss:  393.9398\n",
            "episode: 1200 episode reward: 26.0 eps: 0.877958159110793 avg reward (last 100): 23.574257425742573 episode loss:  578.7548\n",
            "episode: 1300 episode reward: 27.0 eps: 0.8692218948263346 avg reward (last 100): 23.940594059405942 episode loss:  288.56497\n",
            "episode: 1400 episode reward: 18.0 eps: 0.8605725621488737 avg reward (last 100): 24.603960396039604 episode loss:  315.7708\n",
            "episode: 1500 episode reward: 48.0 eps: 0.8520092960514319 avg reward (last 100): 23.861386138613863 episode loss:  475.04654\n",
            "episode: 1600 episode reward: 35.0 eps: 0.843531240114621 avg reward (last 100): 24.07920792079208 episode loss:  251.09732\n",
            "episode: 1700 episode reward: 14.0 eps: 0.8351375464409935 avg reward (last 100): 22.455445544554454 episode loss:  870.29834\n",
            "episode: 1800 episode reward: 36.0 eps: 0.8268273755702407 avg reward (last 100): 21.584158415841586 episode loss:  522.4205\n",
            "episode: 1900 episode reward: 29.0 eps: 0.8185998963952398 avg reward (last 100): 21.415841584158414 episode loss:  577.9524\n",
            "episode: 2000 episode reward: 39.0 eps: 0.8104542860789328 avg reward (last 100): 21.14851485148515 episode loss:  583.77997\n",
            "episode: 2100 episode reward: 53.0 eps: 0.802389729972035 avg reward (last 100): 22.742574257425744 episode loss:  358.37012\n",
            "episode: 2200 episode reward: 23.0 eps: 0.7944054215315619 avg reward (last 100): 24.128712871287128 episode loss:  382.5062\n",
            "episode: 2300 episode reward: 11.0 eps: 0.786500562240163 avg reward (last 100): 21.059405940594058 episode loss:  255.50769\n",
            "episode: 2400 episode reward: 33.0 eps: 0.7786743615262652 avg reward (last 100): 26.514851485148515 episode loss:  231.7114\n",
            "episode: 2500 episode reward: 41.0 eps: 0.7709260366850046 avg reward (last 100): 24.762376237623762 episode loss:  206.89986\n",
            "episode: 2600 episode reward: 18.0 eps: 0.7632548127999497 avg reward (last 100): 21.316831683168317 episode loss:  399.45172\n",
            "episode: 2700 episode reward: 18.0 eps: 0.755659922665597 avg reward (last 100): 24.26732673267327 episode loss:  449.9592\n",
            "episode: 2800 episode reward: 22.0 eps: 0.7481406067106477 avg reward (last 100): 21.92079207920792 episode loss:  184.93669\n",
            "episode: 2900 episode reward: 12.0 eps: 0.7406961129220389 avg reward (last 100): 28.099009900990097 episode loss:  788.19586\n",
            "episode: 3000 episode reward: 10.0 eps: 0.7333256967697342 avg reward (last 100): 24.514851485148515 episode loss:  663.08997\n",
            "episode: 3100 episode reward: 48.0 eps: 0.7260286211322649 avg reward (last 100): 24.77227722772277 episode loss:  271.10162\n",
            "episode: 3200 episode reward: 10.0 eps: 0.7188041562230073 avg reward (last 100): 25.366336633663366 episode loss:  358.86456\n",
            "episode: 3300 episode reward: 19.0 eps: 0.7116515795171979 avg reward (last 100): 23.861386138613863 episode loss:  480.57837\n",
            "episode: 3400 episode reward: 33.0 eps: 0.7045701756796721 avg reward (last 100): 25.02970297029703 episode loss:  373.76846\n",
            "episode: 3500 episode reward: 20.0 eps: 0.6975592364933232 avg reward (last 100): 24.405940594059405 episode loss:  587.52167\n",
            "episode: 3600 episode reward: 16.0 eps: 0.6906180607882736 avg reward (last 100): 26.445544554455445 episode loss:  157.97945\n",
            "episode: 3700 episode reward: 12.0 eps: 0.6837459543717475 avg reward (last 100): 22.287128712871286 episode loss:  413.6948\n",
            "episode: 3800 episode reward: 26.0 eps: 0.6769422299586498 avg reward (last 100): 23.752475247524753 episode loss:  346.53\n",
            "episode: 3900 episode reward: 15.0 eps: 0.6702062071028237 avg reward (last 100): 22.712871287128714 episode loss:  713.9092\n",
            "episode: 4000 episode reward: 34.0 eps: 0.6635372121290035 avg reward (last 100): 24.534653465346533 episode loss:  264.13913\n",
            "episode: 4100 episode reward: 9.0 eps: 0.656934578065436 avg reward (last 100): 22.96039603960396 episode loss:  731.5369\n",
            "episode: 4200 episode reward: 21.0 eps: 0.6503976445771799 avg reward (last 100): 24.11881188118812 episode loss:  730.03625\n",
            "episode: 4300 episode reward: 14.0 eps: 0.6439257579000631 avg reward (last 100): 22.07920792079208 episode loss:  467.4381\n",
            "episode: 4400 episode reward: 15.0 eps: 0.6375182707752978 avg reward (last 100): 21.554455445544555 episode loss:  390.09137\n",
            "episode: 4500 episode reward: 14.0 eps: 0.6311745423847503 avg reward (last 100): 24.04950495049505 episode loss:  629.90295\n",
            "episode: 4600 episode reward: 59.0 eps: 0.6248939382868509 avg reward (last 100): 21.564356435643564 episode loss:  413.2675\n",
            "episode: 4700 episode reward: 17.0 eps: 0.6186758303531436 avg reward (last 100): 24.81188118811881 episode loss:  285.35013\n",
            "episode: 4800 episode reward: 13.0 eps: 0.6125195967054651 avg reward (last 100): 22.89108910891089 episode loss:  96.59467\n",
            "episode: 4900 episode reward: 18.0 eps: 0.6064246216537515 avg reward (last 100): 25.287128712871286 episode loss:  325.8411\n",
            "episode: 5000 episode reward: 12.0 eps: 0.6003902956344622 avg reward (last 100): 22.99009900990099 episode loss:  234.37665\n",
            "episode: 5100 episode reward: 45.0 eps: 0.5944160151496163 avg reward (last 100): 23.84158415841584 episode loss:  462.0387\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import gym\n",
        "import os\n",
        "import datetime\n",
        "from statistics import mean\n",
        "from gym import wrappers\n",
        "\n",
        "from gym.wrappers.record_video import RecordVideo\n",
        "\n",
        "class MyModel(tf.keras.Model):\n",
        "    def __init__(self, num_states, hidden_units, num_actions):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.input_layer = tf.keras.layers.InputLayer(input_shape=(num_states,))\n",
        "        self.hidden_layers = []\n",
        "        for i in hidden_units:\n",
        "            self.hidden_layers.append(tf.keras.layers.Dense(\n",
        "                i, activation='tanh', kernel_initializer='RandomNormal'))\n",
        "        self.output_layer = tf.keras.layers.Dense(\n",
        "            num_actions, activation='linear', kernel_initializer='RandomNormal')\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, inputs):\n",
        "        z = self.input_layer(inputs)\n",
        "        for layer in self.hidden_layers:\n",
        "            z = layer(z)\n",
        "        output = self.output_layer(z)\n",
        "        return output\n",
        "\n",
        "\n",
        "class DQN:\n",
        "    def __init__(self, num_states, num_actions, hidden_units, gamma, max_experiences, min_experiences, batch_size, lr):\n",
        "        self.num_actions = num_actions\n",
        "        self.batch_size = batch_size\n",
        "        self.optimizer = tf.optimizers.Adam(lr)\n",
        "        self.gamma = gamma\n",
        "        self.model = MyModel(num_states, hidden_units, num_actions)\n",
        "        self.experience = {'s': [], 'a': [], 'r': [], 's2': [], 'done': []}\n",
        "        self.max_experiences = max_experiences\n",
        "        self.min_experiences = min_experiences\n",
        "\n",
        "    def predict(self, inputs):\n",
        "        return self.model(np.atleast_2d(inputs.astype('float32')))\n",
        "\n",
        "    def train(self, TargetNet):\n",
        "        if len(self.experience['s']) < self.min_experiences:\n",
        "            return 0\n",
        "        ids = np.random.randint(low=0, high=len(self.experience['s']), size=self.batch_size)\n",
        "        states = np.asarray([self.experience['s'][i] for i in ids])\n",
        "        actions = np.asarray([self.experience['a'][i] for i in ids])\n",
        "        rewards = np.asarray([self.experience['r'][i] for i in ids])\n",
        "        states_next = np.asarray([self.experience['s2'][i] for i in ids])\n",
        "        dones = np.asarray([self.experience['done'][i] for i in ids])\n",
        "        value_next = np.max(TargetNet.predict(states_next), axis=1)\n",
        "        actual_values = np.where(dones, rewards, rewards+self.gamma*value_next)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            selected_action_values = tf.math.reduce_sum(\n",
        "                self.predict(states) * tf.one_hot(actions, self.num_actions), axis=1)\n",
        "            loss = tf.math.reduce_mean(tf.square(actual_values - selected_action_values))\n",
        "        variables = self.model.trainable_variables\n",
        "        gradients = tape.gradient(loss, variables)\n",
        "        self.optimizer.apply_gradients(zip(gradients, variables))\n",
        "        return loss\n",
        "\n",
        "    def get_action(self, states, epsilon):\n",
        "        if np.random.random() < epsilon:\n",
        "            return np.random.choice(self.num_actions)\n",
        "        else:\n",
        "            return np.argmax(self.predict(np.atleast_2d(states))[0])\n",
        "\n",
        "    def add_experience(self, exp):\n",
        "        if len(self.experience['s']) >= self.max_experiences:\n",
        "            for key in self.experience.keys():\n",
        "                self.experience[key].pop(0)\n",
        "        for key, value in exp.items():\n",
        "            self.experience[key].append(value)\n",
        "\n",
        "    def copy_weights(self, TrainNet):\n",
        "        variables1 = self.model.trainable_variables\n",
        "        variables2 = TrainNet.model.trainable_variables\n",
        "        for v1, v2 in zip(variables1, variables2):\n",
        "            v1.assign(v2.numpy())\n",
        "\n",
        "\n",
        "def play_game(env, TrainNet, TargetNet, epsilon, copy_step):\n",
        "    rewards = 0\n",
        "    iter = 0\n",
        "    done = False\n",
        "    observations = env.reset()\n",
        "    losses = list()\n",
        "    while not done:\n",
        "        action = TrainNet.get_action(observations, epsilon)\n",
        "        prev_observations = observations\n",
        "        observations, reward, done, _ = env.step(action)\n",
        "        rewards += reward\n",
        "        if done:\n",
        "            reward = -200\n",
        "            env.reset()\n",
        "\n",
        "        exp = {'s': prev_observations, 'a': action, 'r': reward, 's2': observations, 'done': done}\n",
        "        TrainNet.add_experience(exp)\n",
        "        loss = TrainNet.train(TargetNet)\n",
        "        if isinstance(loss, int):\n",
        "            losses.append(loss)\n",
        "        else:\n",
        "            losses.append(loss.numpy())\n",
        "        iter += 1\n",
        "        if iter % copy_step == 0:\n",
        "            TargetNet.copy_weights(TrainNet)\n",
        "    return rewards, mean(losses)\n",
        "\n",
        "def make_video(env, TrainNet):\n",
        "\n",
        "    env = RecordVideo(env, os.path.join(os.getcwd(), \"videos\"+str(i)),  episode_trigger = lambda episode_number: True)\n",
        "\n",
        "    rewards = 0\n",
        "    steps = 0\n",
        "    done = False\n",
        "    observation = env.reset()\n",
        "    while not done:\n",
        "        env.render()\n",
        "        action = TrainNet.get_action(observation, 0)\n",
        "        observation, reward, done, _ = env.step(action)\n",
        "        steps += 1\n",
        "        rewards += reward\n",
        "    print(\"Testing steps: {} rewards {}: \".format(steps, rewards))\n",
        "\n",
        "\n",
        "def main():\n",
        "    env = gym.make('CartPole-v0')\n",
        "    gamma = 0.99\n",
        "    copy_step = 25\n",
        "    num_states = len(env.observation_space.sample())\n",
        "    num_actions = env.action_space.n\n",
        "    hidden_units = [200, 200]\n",
        "    max_experiences = 10000\n",
        "    min_experiences = 100\n",
        "    batch_size = 32\n",
        "    lr = 1e-2\n",
        "    current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "    log_dir = 'logs/dqn/' + current_time\n",
        "    summary_writer = tf.summary.create_file_writer(log_dir)\n",
        "\n",
        "    TrainNet = DQN(num_states, num_actions, hidden_units, gamma, max_experiences, min_experiences, batch_size, lr)\n",
        "    TargetNet = DQN(num_states, num_actions, hidden_units, gamma, max_experiences, min_experiences, batch_size, lr)\n",
        "    N = 10000 #50000\n",
        "    total_rewards = np.empty(N)\n",
        "    epsilon = 0.99\n",
        "    decay = 0.9999\n",
        "    min_epsilon = 0.1\n",
        "    for n in range(N):\n",
        "        epsilon = max(min_epsilon, epsilon * decay)\n",
        "        total_reward, losses = play_game(env, TrainNet, TargetNet, epsilon, copy_step)\n",
        "        total_rewards[n] = total_reward\n",
        "        avg_rewards = total_rewards[max(0, n - 100):(n + 1)].mean()\n",
        "        with summary_writer.as_default():\n",
        "            tf.summary.scalar('episode reward', total_reward, step=n)\n",
        "            tf.summary.scalar('running avg reward(100)', avg_rewards, step=n)\n",
        "            tf.summary.scalar('average loss)', losses, step=n)\n",
        "        if n % 100 == 0:\n",
        "            print(\"episode:\", n, \"episode reward:\", total_reward, \"eps:\", epsilon, \"avg reward (last 100):\", avg_rewards,\n",
        "                  \"episode loss: \", losses)\n",
        "    print(\"avg reward for last 100 episodes:\", avg_rewards)\n",
        "    make_video(env, TrainNet)\n",
        "    env.close()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    for i in range(3):\n",
        "        main()"
      ]
    }
  ]
}